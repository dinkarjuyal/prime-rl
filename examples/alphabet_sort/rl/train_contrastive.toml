max_steps = 100

[model]
name = "Qwen/Qwen3-4B-Instruct-2507"
impl = "liger_kernel"

[model.ac]
freq = 1

[model.experimental.lora]
rank = 32
alpha = 64
dropout = 0.0
target_modules = [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj"
]
modules_to_save = [
    "embed_tokens",
    "norm",
    "layernorm",
    "lm_head$"
]

[optim]
lr = 1e-5

[loss]
ratio_type = "sequence"
ratio_length_norm = true
# Enable contrastive loss with linear scheduling: 0.05 â†’ 0.5
contrastive_loss_weight = 0.05  # Start low when advantages are noisy
contrastive_loss_weight_end = 0.5  # Ramp up as training stabilizes
contrastive_loss_type = "infonce"
contrastive_temperature = 0.1

[wandb]
project = "prime-rl"
name = "alphabet-sort-4b-contrastive"

